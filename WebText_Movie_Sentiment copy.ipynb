{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cc29357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "545d18ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "veryNegative = []\n",
    "Negative = []\n",
    "Positive = []\n",
    "veryPositive = []\n",
    "data_X = \"\"\n",
    "data_Y = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "803c81f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateStopWordList():\n",
    "\n",
    "    #Fetch stopwords.txt file path\n",
    "    stopWords_dataset = dirPath+\"/Data/stopwords.txt\"\n",
    "\n",
    "    #stopwords List\n",
    "    stopWords = []\n",
    "\n",
    "    #Open the stopwords txt file read the data and store in a list\n",
    "    try:\n",
    "        fp = open(stopWords_dataset, 'r')\n",
    "        line = fp.readline()\n",
    "        while line:\n",
    "            word = line.strip()\n",
    "            stopWords.append(word)\n",
    "            line = fp.readline()\n",
    "        fp.close()\n",
    "    except:\n",
    "        print(\"ERROR: Opening File\")\n",
    "\n",
    "    return stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5767076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_AffinityList(datasetLink):\n",
    "\n",
    "    affin_dataset = datasetLink\n",
    "    try:\n",
    "        affin_list = open(affin_dataset).readlines()\n",
    "    except:\n",
    "        print(\"\", affin_dataset)\n",
    "        exit(0)\n",
    "    \n",
    "    return affin_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94578abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDictionaryFromPolarity(affin_list):\n",
    "\n",
    "    # Create list to store the words and its polarity score\n",
    "    words = []\n",
    "    score = []\n",
    "\n",
    "    # Iterate and assign words and thier polarities\n",
    "    for word in affin_list:\n",
    "        words.append(word.split(\"\\t\")[0].lower())\n",
    "        score.append(int(word.split(\"\\t\")[1].split(\"\\n\")[0]))\n",
    "\n",
    "    #Categorize words into different categories based on polarities\n",
    "    for elem in range(len(words)):\n",
    "        if score[elem] == -4 or score[elem] == -5:\n",
    "            veryNegative.append(words[elem])\n",
    "        elif score[elem] == -3 or score[elem] == -2 or score[elem] == -1:\n",
    "            Negative.append(words[elem])\n",
    "        elif score[elem] == 3 or score[elem] == 2 or score[elem] == 1:\n",
    "            Positive.append(words[elem])\n",
    "        elif score[elem] == 4 or score[elem] == 5:\n",
    "            veryPositive.append(words[elem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c233fc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dataSet):\n",
    "\n",
    "    processed_data = []\n",
    "\n",
    "    #Make a list of all the Stopwords to be removed\n",
    "    stopWords = generateStopWordList()\n",
    "\n",
    "    #For every TWEET in the dataset do,\n",
    "    for tweet in dataSet:\n",
    "\n",
    "        temp_tweet = tweet\n",
    "\n",
    "        #Convert @username to USER_MENTION\n",
    "        tweet = re.sub('@[^\\s]+','USER_MENTION',tweet).lower()\n",
    "        tweet.replace(temp_tweet, tweet)\n",
    "\n",
    "        #Remove the unnecessary white spaces\n",
    "        tweet = re.sub('[\\s]+',' ', tweet)\n",
    "        tweet.replace(temp_tweet,tweet)\n",
    "\n",
    "        #Replace HASH (#) symbol in hastag\n",
    "        tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "\n",
    "        #Replace all the numeric terms\n",
    "        tweet = re.sub('[0-9]+', \"\",tweet)\n",
    "        tweet.replace(temp_tweet,tweet)\n",
    "\n",
    "        #Remove all the STOP WORDS\n",
    "        for sw in stopWords:\n",
    "            if sw in tweet:\n",
    "                tweet = re.sub(r'\\b' + sw + r'\\b'+\" \",\"\",tweet)\n",
    "\n",
    "        tweet.replace(temp_tweet, tweet)\n",
    "\n",
    "        #Replace all Punctuations\n",
    "        tweet = re.sub('[^a-zA-z ]',\"\",tweet)\n",
    "        tweet.replace(temp_tweet,tweet)\n",
    "\n",
    "        #Remove additional white spaces\n",
    "        tweet = re.sub('[\\s]+',' ', tweet)\n",
    "        tweet.replace(temp_tweet,tweet)\n",
    "\n",
    "        #Save the Processed Tweet after data cleansing\n",
    "        processed_data.append(tweet)\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fae50722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeaturizeTrainingData(dataset, type_class):\n",
    "\n",
    "    neutral_list = []\n",
    "    i=0\n",
    "\n",
    "    # For each Tweet split the Tweet by \" \" (space) i.e. split every word of the Tweet\n",
    "    data = [tweet.strip().split(\" \") for tweet in dataset]\n",
    "    #print(data)\n",
    "\n",
    "    # Feature Vector is to store the feature of the TWEETs\n",
    "    feature_vector = []\n",
    "\n",
    "    # for every sentence i.e. TWEET find the words and their category\n",
    "    for sentence in data:\n",
    "        # Category count for every Sentence or TWEET\n",
    "        veryNegative_count = 0\n",
    "        Negative_count = 0\n",
    "        Positive_count = 0\n",
    "        veryPositive_count = 0\n",
    "\n",
    "        # for every word in sentence, categorize\n",
    "        # and increment the count by 1 if found\n",
    "        for word in sentence:\n",
    "            if word in veryPositive:\n",
    "                veryPositive_count = veryPositive_count + 1\n",
    "            elif word in Positive:\n",
    "                Positive_count = Positive_count + 1\n",
    "            elif word in veryNegative:\n",
    "                veryNegative_count = veryNegative_count + 1\n",
    "            elif word in Negative:\n",
    "                Negative_count = Negative_count + 1\n",
    "        i+=1\n",
    "\n",
    "        #Assign Class Label\n",
    "        if veryPositive_count == veryNegative_count == Positive_count == Negative_count:\n",
    "            feature_vector.append([veryPositive_count, Positive_count, Negative_count, veryNegative_count, \"neutral\"])\n",
    "            neutral_list.append(i)\n",
    "        else:\n",
    "            feature_vector.append([veryPositive_count, Positive_count, Negative_count, veryNegative_count, type_class])\n",
    "\n",
    "    #print(neutral_list)\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25bce923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatureizeTestData(dataset):\n",
    "\n",
    "    data = [tweet.strip().split(\" \") for tweet in dataset]\n",
    "    #print(data)\n",
    "    count_Matrix = []\n",
    "    feature_vector = []\n",
    "\n",
    "    for sentence in data:\n",
    "        veryNegative_count = 0\n",
    "        Negative_count = 0\n",
    "        Positive_count = 0\n",
    "        veryPositive_count = 0\n",
    "\n",
    "        # for every word in sentence, categorize\n",
    "        # and increment the count by 1 if found\n",
    "        for word in sentence:\n",
    "            if word in veryPositive:\n",
    "                veryPositive_count = veryPositive_count + 1\n",
    "            elif word in Positive:\n",
    "                Positive_count = Positive_count + 1\n",
    "            elif word in veryNegative:\n",
    "                veryNegative_count = veryNegative_count + 1\n",
    "            elif word in Negative:\n",
    "                Negative_count = Negative_count + 1\n",
    "\n",
    "        if (veryPositive_count + Positive_count) > (veryNegative_count + Negative_count):\n",
    "            feature_vector.append([veryPositive_count, Positive_count, Negative_count, veryNegative_count, \"positive\"])\n",
    "            #neutral_list.append(i)\n",
    "        elif (veryPositive_count + Positive_count) < (veryNegative_count + Negative_count):\n",
    "            feature_vector.append([veryPositive_count, Positive_count, Negative_count, veryNegative_count, \"negative\"])\n",
    "        else:\n",
    "            feature_vector.append([veryPositive_count, Positive_count, Negative_count, veryNegative_count, \"neutral\"])\n",
    "\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f58976d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_naive_bayes(train_X, train_Y, test_X):\n",
    "\n",
    "    print(\"Classifying using Gaussian Naive Bayes ...\")\n",
    "\n",
    "    gnb = GaussianNB()\n",
    "    yPred = gnb.fit(train_X,train_Y).predict(test_X)\n",
    "\n",
    "    return yPred\n",
    "\n",
    "def classify_svm(train_X, train_Y, test_X):\n",
    "\n",
    "    print(\"Classifying using Support Vector Machine ...\")\n",
    "\n",
    "    clf = SVC()\n",
    "    clf.fit(train_X,train_Y)\n",
    "    yPred = clf.predict(test_X)\n",
    "\n",
    "    return yPred\n",
    "\n",
    "def classify_maxEnt(train_X, train_Y, test_X):\n",
    "\n",
    "    print(\"Classifying using Maximum Entropy ...\")\n",
    "    maxEnt = LogisticRegressionCV()\n",
    "    maxEnt.fit(train_X, train_Y)\n",
    "    yPred = maxEnt.predict(test_X)\n",
    "\n",
    "    return yPred\n",
    "\n",
    "\n",
    "#########FOR TEST DATA CLASSIFICATION########\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2928f03",
   "metadata": {},
   "source": [
    "# For Twitter test data classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b66f4d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_naive_bayes_twitter(train_X, train_Y, test_X, test_Y):\n",
    "\n",
    "    print(\"Classifying using Gaussian Naive Bayes Algorithm...\")\n",
    "    gnb = GaussianNB()\n",
    "    yPred = gnb.fit(train_X,train_Y).predict(test_X)\n",
    "    import pandas as pd\n",
    "    pd.DataFrame(yPred).to_csv(\"/Users/vineethkumarrenukuntla/Desktop/vinks/Data/PredictionsforMortalKombat/gnbpredfile.csv\")\n",
    "    conf_mat = confusion_matrix(test_Y,yPred)\n",
    "    print(conf_mat)\n",
    "    Accuracy = (sum(conf_mat.diagonal())) / np.sum(conf_mat)\n",
    "    print(\"Accuray: \", Accuracy)\n",
    "    evaluate_classifier(conf_mat)\n",
    "\n",
    "\n",
    "def classify_svm_twitter(train_X, train_Y, test_X, test_Y):\n",
    "\n",
    "    print(\"Classifying using Support Vector Machine Algorithm...\")\n",
    "    clf = SVC()\n",
    "    clf.fit(train_X,train_Y)\n",
    "    yPred = clf.predict(test_X)\n",
    "    import pandas as pd\n",
    "    pd.DataFrame(yPred).to_csv(\"/Users/vineethkumarrenukuntla/Desktop/vinks/Data/PredictionsforMortalKombat/svmpredfile.csv\")\n",
    "    conf_mat = confusion_matrix(test_Y,yPred)\n",
    "    print(conf_mat)\n",
    "    Accuracy = (sum(conf_mat.diagonal())) / np.sum(conf_mat)\n",
    "    print(\"Accuracy: \", Accuracy)\n",
    "    evaluate_classifier(conf_mat)\n",
    "\n",
    "def classify_maxEnt_twitter(train_X, train_Y, test_X, test_Y):\n",
    "\n",
    "    print(\"Classifying using Maximum Entropy Algorithm...\")\n",
    "    maxEnt = LogisticRegressionCV()\n",
    "    maxEnt.fit(train_X, train_Y)\n",
    "    yPred = maxEnt.predict(test_X)\n",
    "    import pandas as pd\n",
    "    #Downloading the prediction to CSV file\n",
    "    pd.DataFrame(yPred).to_csv(\"/Users/vineethkumarrenukuntla/Desktop/vinks/Data/PredictionsforMortalKombat/maxEntpredfile.csv\")\n",
    "    conf_mat = confusion_matrix(test_Y,yPred)\n",
    "    print(conf_mat)\n",
    "    Accuracy = (sum(conf_mat.diagonal())) / np.sum(conf_mat)\n",
    "    print(\"Accuracy: \", Accuracy)\n",
    "    evaluate_classifier(conf_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8683abc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_twitter_data(file_name):\n",
    "\n",
    "    test_data = open(dirPath+\"/vinks/Data/\"+file_name, encoding=\"utf8\").readlines()\n",
    "    test_data = preprocessing(test_data)\n",
    "    test_data = FeatureizeTestData(test_data)\n",
    "    test_data = np.reshape(np.asarray(test_data),newshape=(len(test_data),5))\n",
    "    print(len(test_data))\n",
    "\n",
    "    #Split Data into Features and Classes\n",
    "    data_X_test = test_data[:,:4].astype(int)\n",
    "    data_Y_test = test_data[:,4]\n",
    "\n",
    "    print(\"Classifying\",)\n",
    "    classify_naive_bayes_twitter(data_X, data_Y, data_X_test, data_Y_test)\n",
    "    classify_svm_twitter(data_X, data_Y, data_X_test, data_Y_test)\n",
    "    classify_maxEnt_twitter(data_X, data_Y, data_X_test, data_Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31ed1937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(conf_mat):\n",
    "    Precision = conf_mat[0,0]/(sum(conf_mat[0]))\n",
    "    Recall = conf_mat[0,0] / (sum(conf_mat[:,0]))\n",
    "    F_Measure = (2 * (Precision * Recall))/ (Precision + Recall)\n",
    "\n",
    "    print(\"Precision: \",Precision)\n",
    "    print(\"Recall: \", Recall)\n",
    "    print(\"F-Measure: \", F_Measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1db12c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')        #!!!!!IMPORTANT UNCOMMENT\n",
    "dirPath = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b863b56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait while we Classify your data ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Please wait while we Classify your data ...\")\n",
    "affin_list = generate_AffinityList(dirPath+\"/vinks/Data/Affin_Data.txt\")\n",
    "createDictionaryFromPolarity(affin_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "247cf5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing in progress !\n",
      "ERROR: Opening File\n",
      "ERROR: Opening File\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing in progress !\")\n",
    "negative_data = open(dirPath+\"/vinks/Data/Rt-polarity-neg.txt\").readlines()\n",
    "positive_data = open(dirPath+\"/vinks/Data/Rt-polarity-pos.txt\").readlines()\n",
    "positive_data = preprocessing(positive_data)\n",
    "negative_data = preprocessing(negative_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cc41dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the Feature Vectors ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating the Feature Vectors ...\")\n",
    "positive_sentiment = FeaturizeTrainingData(positive_data, \"positive\")\n",
    "negative_sentiment = FeaturizeTrainingData(negative_data,\"negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c216843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = positive_sentiment + negative_sentiment\n",
    "final_data = np.reshape(np.asarray(final_data),newshape=(len(final_data),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90c4a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = final_data[:,:4].astype(int)\n",
    "data_Y = final_data[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d53f540d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Classifer according to the data provided ...\n",
      "Classifying the Test Data ...\n",
      "Evaluation Results will be displayed Shortly ...\n",
      "Classifying using Gaussian Naive Bayes ...\n",
      "[[2297    0 2239]\n",
      " [   1 1488    0]\n",
      " [ 843    0 3792]]\n",
      "Accuracy:  0.7107879924953096\n",
      "Precision:  0.5063932980599647\n",
      "Recall:  0.7312957656797199\n",
      "F-Measure:  0.5984108375667578\n",
      "Classifying using Support Vector Machine ...\n",
      "[[3013    0 1523]\n",
      " [   0 1488    1]\n",
      " [1362    0 3273]]\n",
      "Accuracy:  0.7292682926829268\n",
      "Precision:  0.6642416225749559\n",
      "Recall:  0.6886857142857142\n",
      "F-Measure:  0.676242845920772\n",
      "Classifying using Maximum Entropy ...\n",
      "[[2992    0 1544]\n",
      " [   1 1488    0]\n",
      " [1353    0 3282]]\n",
      "Accuracy:  0.7281425891181988\n",
      "Precision:  0.6596119929453262\n",
      "Recall:  0.6884491486424298\n",
      "F-Measure:  0.673722134654357\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the Classifer according to the data provided ...\")\n",
    "print(\"Classifying the Test Data ...\")\n",
    "print(\"Evaluation Results will be displayed Shortly ...\")\n",
    "\n",
    "yPred = classify_naive_bayes(data_X, data_Y, data_X)\n",
    "conf_mat = confusion_matrix(data_Y, yPred)\n",
    "print(conf_mat)\n",
    "Accuracy = (sum(conf_mat.diagonal())) / np.sum(conf_mat)\n",
    "print(\"Accuracy: \", Accuracy)\n",
    "evaluate_classifier(conf_mat)\n",
    "\n",
    "yPred = classify_svm(data_X, data_Y, data_X)\n",
    "conf_mat = confusion_matrix(data_Y, yPred)\n",
    "print(conf_mat)\n",
    "Accuracy = (sum(conf_mat.diagonal())) / np.sum(conf_mat)\n",
    "print(\"Accuracy: \", Accuracy)\n",
    "evaluate_classifier(conf_mat)\n",
    "\n",
    "yPred = classify_maxEnt(data_X, data_Y, data_X)\n",
    "conf_mat = confusion_matrix(data_Y, yPred)\n",
    "print(conf_mat)\n",
    "Accuracy = (sum(conf_mat.diagonal())) / np.sum(conf_mat)\n",
    "print(\"Accuracy: \", Accuracy)\n",
    "evaluate_classifier(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650fae75",
   "metadata": {},
   "source": [
    "# Using trained models to depict the sentiment in Movie Tweets (#MortalKombatmovie) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c42b69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Opening File\n",
      "40007\n",
      "Classifying\n",
      "Classifying using Gaussian Naive Bayes Algorithm...\n",
      "[[ 6614     0   131]\n",
      " [  933 16449  2084]\n",
      " [  382     0 13414]]\n",
      "Accuray:  0.9117654410478166\n",
      "Precision:  0.9805782060785767\n",
      "Recall:  0.8341531088409635\n",
      "F-Measure:  0.9014583617282267\n",
      "Classifying using Support Vector Machine Algorithm...\n",
      "[[ 6639     0   106]\n",
      " [ 2547 16449   470]\n",
      " [  271     0 13525]]\n",
      "Accuracy:  0.9151648461519234\n",
      "Precision:  0.9842846553002224\n",
      "Recall:  0.7020196679708153\n",
      "F-Measure:  0.8195284532773731\n",
      "Classifying using Maximum Entropy Algorithm...\n",
      "[[ 6744     0     1]\n",
      " [ 2587 16449   430]\n",
      " [   71     0 13725]]\n",
      "Accuracy:  0.9227885120103981\n",
      "Precision:  0.9998517420311341\n",
      "Recall:  0.7172941927249521\n",
      "F-Measure:  0.8353254474515389\n"
     ]
    }
   ],
   "source": [
    " classify_twitter_data(file_name=\"MortalKombattweets40K.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d71ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
